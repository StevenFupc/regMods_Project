The Effect of Transmission on Fuel Economy
==========================================

The purpose of this analysis is to quantify the relationship between vehicle
transmission and fuel economy, measured in miles per gallon (mpg). The data 
we use is taken from the `mtcars` dataset included in the R programming language
. This data is taken from the 1974 Motor Trend magazine, and details 11 aspects
of motor vehicle design and performance, including:

1. Fuel economy (in miles per gallon)
2. Number of cylinders
3. Engine displacement (in cubic inches)
4. Horsepower
5. Rear axle ratio
6. Weight (in thousands of pounds)
7. Quarter mile time (in seconds)
8. Cylinder arrangement (either V or straight)
9. Transmission (either automatic or manual)
10. Number of forward gears
11. Number of carburetors

First, we load the `ggplot2` graphics library and the `mtcars` dataset.
```{r}
library(ggplot2)
data(mtcars)
```

Next, we examine the pairwise relationship between the variables.
```{r}
plot(mtcars)
```
We find that there is a slight difference in fuel economy between manual and
automatic transmissions. We also see that there is a fair amount of covariation
in the columns. This can be problematic for linear regression, as omitting
interaction terms can make models less accurate, but including these terms makes
linear models much harder to interpret.

We will fit a linear model in a stepwise fashion, starting with transmission
type. To maintain the interpretability of our model, we will not include
interaction terms. To limit the effect of the corelation between variables, we 
will limit our model to 3 predictors.

First, we create a one-predictor model based on transmission type.
```{r}
mpg_am <- lm(mpg ~ am, data=mtcars)
summary(mpg_am)

ggplot(mtcars, aes(x=am, y=mpg)) +
    geom_point() +
    geom_abline(intercept=17, slope=7.2)
```
We see that there is a definite separation between the means of the manual and automatic cars. However, our model does not do a good job of separating our values; the R squared
is only 0.338.

We will use R squared to determine which predictor to add next. The 
```{r}
variablesLeft <- setdiff(names(mtcars), c("mpg", "am"))

models1 <- lapply(variablesLeft, FUN=function(x) {
    form <- paste0("mpg ~ am + ", x)
    lm(form, data=mtcars)
})

cbind(variablesLeft, adj.r.squared=sapply(models1, FUN=function(x) summary(x)$adj.r.squared))

summary(models1[[3]])

ggplot(mtcars, aes(x=hp, y=mpg, color=as.factor(am))) +
    geom_point()
```
# Here, we find that HP is the best independent covariate. Let's find one more:

```{r}
variablesLeft <- setdiff(variablesLeft, "hp")
models2 <- lapply(variablesLeft, FUN=function(x) {
    form <- paste0("mpg ~ am + hp + ", x)
    lm(form, data=mtcars)
})

cbind(variablesLeft, adj.r.squared=sapply(models2, FUN=function(x) summary(x)$adj.r.squared))
```



# And our winner is weight; let's take a look at the fit now.
```{r}
summary(models2[[4]])

ggplot(mtcars, aes(x=hp, y=mpg, color=as.factor(am))) +
    geom_point(aes(size=wt))

plot(models2[[4]])
```

